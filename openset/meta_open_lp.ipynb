{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys  \n",
    "sys.path.insert(1, '/home/manying/Projects/fcgFewShot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_config\n",
    "options = load_config(\"../config/config_label_prop_openset_meta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all datasets...\n",
      "train dataset shape: (1180, 16)\n",
      "train dataset family number: 59\n",
      "test dataset shape: (200, 16)\n",
      "test dataset family number: 10\n",
      "val dataset shape: (200, 16)\n",
      "val dataset family number: 10\n",
      "Loading openset data...\n",
      "Openset data shape: (171, 16)\n"
     ]
    }
   ],
   "source": [
    "from loadDataset import LoadDataset\n",
    "from fcgVectorize import FCGVectorize\n",
    "from trainModule import TrainModule\n",
    "from trainModule import TestModule\n",
    "import os\n",
    "\n",
    "dataset = LoadDataset(options, pretrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the training module...\n",
      "Loading data from ../embeddings/x86_64_withVal_withPretrain_ghidra_888/word2vec...\n",
      "Loading training data...\n",
      "Loading data from ../embeddings/x86_64_withVal_withPretrain_ghidra_888/word2vec/trainData.pkl...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data...\n",
      "Loading data from ../embeddings/x86_64_withVal_withPretrain_ghidra_888/word2vec/valData.pkl...\n",
      "Device: cuda:0\n",
      "Model: LabelPropagation(\n",
      "  (encoder): GraphSAGELayer(\n",
      "    (sage_convs): ModuleList(\n",
      "      (0-2): 3 x SAGEConv(128, 128, aggr=mean)\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (relation): GraphRelationNetwork(\n",
      "    (sage): GraphSAGELayer(\n",
      "      (sage_convs): ModuleList(\n",
      "        (0): SAGEConv(128, 64, aggr=mean)\n",
      "        (1): SAGEConv(64, 32, aggr=mean)\n",
      "      )\n",
      "      (norms): ModuleList(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Loss function: LabelPropagation(\n",
      "  (encoder): GraphSAGELayer(\n",
      "    (sage_convs): ModuleList(\n",
      "      (0-2): 3 x SAGEConv(128, 128, aggr=mean)\n",
      "    )\n",
      "    (norms): ModuleList(\n",
      "      (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (relation): GraphRelationNetwork(\n",
      "    (sage): GraphSAGELayer(\n",
      "      (sage_convs): ModuleList(\n",
      "        (0): SAGEConv(128, 64, aggr=mean)\n",
      "        (1): SAGEConv(64, 32, aggr=mean)\n",
      "      )\n",
      "      (norms): ModuleList(\n",
      "        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Finish setting up the training module\n"
     ]
    }
   ],
   "source": [
    "trainModule = TrainModule(options, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([18,  1,  4,  7, 22, 42, 29, 56, 20, 52]), [tensor([ 6,  7,  9, 11, 12]), tensor([27, 31, 32, 37, 49]), tensor([ 5, 18, 50, 51, 52]), tensor([ 3, 13, 14, 41, 42]), tensor([ 0, 23, 26, 55, 62])], tensor([ 29,  30,  45,  61,  64,  71,  80,  81, 106, 115, 144, 155, 186, 188,\n",
      "        194,  65,  73,  92, 101, 103, 116, 122, 123, 133, 153, 156, 165, 169,\n",
      "        181, 185,  56,  74,  76,  84, 111, 112, 118, 121, 127, 148, 154, 158,\n",
      "        164, 171, 195,  46,  66,  87,  88,  93, 107, 134, 146, 170, 173, 178,\n",
      "        180, 184, 190, 199,  69,  75,  78,  99, 117, 120, 136, 137, 138, 141,\n",
      "        142, 149, 176, 192, 196]), tensor([ 21,  35,  47,  48,  79,  89,  94,  95,  96,  97, 102, 110, 113, 125,\n",
      "        130, 139, 150, 175, 197, 198,   2,  10,  15,  16,  39,  40,  44,  60,\n",
      "         70,  72,  83,  86, 100, 108, 119, 126, 129, 152, 161, 163,   4,  19,\n",
      "         24,  33,  59,  63,  67,  77,  82, 105, 109, 124, 157, 159, 166, 168,\n",
      "        177, 182, 183, 193,   1,  20,  28,  38,  54,  57,  85, 104, 131, 135,\n",
      "        145, 147, 160, 162, 167, 174, 179, 187, 189, 191,   8,  17,  22,  25,\n",
      "         34,  36,  43,  53,  58,  68,  90,  91,  98, 114, 128, 132, 140, 143,\n",
      "        151, 172]))\n"
     ]
    }
   ],
   "source": [
    "for data in trainModule.trainLoader:\n",
    "    print(trainModule.loss_fn.get_support_query_idxs(data.y, openset=True, cls_support=5))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prototype",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
