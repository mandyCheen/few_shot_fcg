2025-06-05 17:06:41,543 - process_2 - INFO - Process 2 started on cuda:2
2025-06-05 17:06:41,641 - process_2 - INFO - Process 2 - Task 1/45 - seed: 31, exp: 10way_5shot, lambda: 0.1
2025-06-05 17:06:41,788 - process_2 - INFO - Process 2 - Loading dataset
2025-06-05 17:06:41,813 - process_2 - INFO - Process 2 - Starting training
2025-06-05 23:38:55,226 - process_2 - INFO - Process 2 - Starting testing
2025-06-05 23:57:53,017 - process_2 - INFO - Process 2 - Task 1/45 completed successfully
2025-06-05 23:57:53,018 - process_2 - INFO - Process 2 - Task 2/45 - seed: 31, exp: 10way_5shot, lambda: 0.2
2025-06-05 23:57:53,139 - process_2 - INFO - Process 2 - Starting training
2025-06-06 06:36:10,565 - process_2 - INFO - Process 2 - Starting testing
2025-06-06 06:55:12,366 - process_2 - INFO - Process 2 - Task 2/45 completed successfully
2025-06-06 06:55:12,366 - process_2 - INFO - Process 2 - Task 3/45 - seed: 31, exp: 10way_5shot, lambda: 0.3
2025-06-06 06:55:12,478 - process_2 - INFO - Process 2 - Starting training
2025-06-06 08:15:56,854 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_5shot, lambda=0.3): CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 14.53 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 12.62 GiB memory in use. Of the allocated memory 7.60 GiB is allocated by PyTorch, and 4.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 08:15:56,855 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.71 GiB. GPU 2 has a total capacity of 14.53 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 12.62 GiB memory in use. Of the allocated memory 7.60 GiB is allocated by PyTorch, and 4.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 08:15:57,105 - process_2 - INFO - Process 2 - Task 4/45 - seed: 31, exp: 10way_5shot, lambda: 0.4
2025-06-06 08:15:57,594 - process_2 - INFO - Process 2 - Starting training
2025-06-06 11:17:56,670 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_5shot, lambda=0.4): CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 14.53 GiB of which 1.85 GiB is free. Including non-PyTorch memory, this process has 12.67 GiB memory in use. Of the allocated memory 7.56 GiB is allocated by PyTorch, and 4.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 11:17:56,671 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 2 has a total capacity of 14.53 GiB of which 1.85 GiB is free. Including non-PyTorch memory, this process has 12.67 GiB memory in use. Of the allocated memory 7.56 GiB is allocated by PyTorch, and 4.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 11:17:56,992 - process_2 - INFO - Process 2 - Task 5/45 - seed: 31, exp: 10way_5shot, lambda: 0.6
2025-06-06 11:17:57,309 - process_2 - INFO - Process 2 - Starting training
2025-06-06 14:04:05,496 - process_2 - INFO - Process 2 - Starting testing
2025-06-06 14:22:55,613 - process_2 - INFO - Process 2 - Task 5/45 completed successfully
2025-06-06 14:22:55,613 - process_2 - INFO - Process 2 - Task 6/45 - seed: 31, exp: 10way_5shot, lambda: 0.7
2025-06-06 14:22:55,782 - process_2 - INFO - Process 2 - Starting training
2025-06-06 16:34:02,881 - process_2 - INFO - Process 2 - Starting testing
2025-06-06 16:53:08,731 - process_2 - INFO - Process 2 - Task 6/45 completed successfully
2025-06-06 16:53:08,732 - process_2 - INFO - Process 2 - Task 7/45 - seed: 31, exp: 10way_5shot, lambda: 0.8
2025-06-06 16:53:08,849 - process_2 - INFO - Process 2 - Starting training
2025-06-06 17:18:49,748 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_5shot, lambda=0.8): CUDA out of memory. Tried to allocate 3.97 GiB. GPU 2 has a total capacity of 14.53 GiB of which 2.90 GiB is free. Including non-PyTorch memory, this process has 11.62 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 3.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 17:18:49,749 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.97 GiB. GPU 2 has a total capacity of 14.53 GiB of which 2.90 GiB is free. Including non-PyTorch memory, this process has 11.62 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 3.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 17:18:50,146 - process_2 - INFO - Process 2 - Task 8/45 - seed: 31, exp: 10way_5shot, lambda: 0.9
2025-06-06 17:18:50,490 - process_2 - INFO - Process 2 - Starting training
2025-06-06 21:01:57,353 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_5shot, lambda=0.9): CUDA out of memory. Tried to allocate 3.77 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.24 GiB is free. Including non-PyTorch memory, this process has 11.28 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 3.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 21:01:57,355 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.77 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.24 GiB is free. Including non-PyTorch memory, this process has 11.28 GiB memory in use. Of the allocated memory 7.77 GiB is allocated by PyTorch, and 3.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 21:01:57,872 - process_2 - INFO - Process 2 - Task 9/45 - seed: 31, exp: 10way_5shot, lambda: 1.0
2025-06-06 21:01:58,376 - process_2 - INFO - Process 2 - Starting training
2025-06-07 01:23:56,978 - process_2 - INFO - Process 2 - Starting testing
2025-06-07 01:43:21,571 - process_2 - INFO - Process 2 - Task 9/45 completed successfully
2025-06-07 01:43:21,571 - process_2 - INFO - Process 2 - Task 10/45 - seed: 31, exp: 10way_10shot, lambda: 0.1
2025-06-07 01:43:21,692 - process_2 - INFO - Process 2 - Starting training
2025-06-07 02:30:15,561 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_10shot, lambda=0.1): CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.53 GiB is free. Including non-PyTorch memory, this process has 10.99 GiB memory in use. Of the allocated memory 7.30 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 02:30:15,562 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.53 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.53 GiB is free. Including non-PyTorch memory, this process has 10.99 GiB memory in use. Of the allocated memory 7.30 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 02:30:16,121 - process_2 - INFO - Process 2 - Task 11/45 - seed: 31, exp: 10way_10shot, lambda: 0.2
2025-06-07 02:30:16,616 - process_2 - INFO - Process 2 - Starting training
2025-06-07 05:22:26,225 - process_2 - INFO - Process 2 - Starting testing
2025-06-07 05:41:33,107 - process_2 - INFO - Process 2 - Task 11/45 completed successfully
2025-06-07 05:41:33,107 - process_2 - INFO - Process 2 - Task 12/45 - seed: 31, exp: 10way_10shot, lambda: 0.3
2025-06-07 05:41:33,260 - process_2 - INFO - Process 2 - Starting training
2025-06-07 08:22:47,327 - process_2 - INFO - Process 2 - Starting testing
2025-06-07 08:42:02,901 - process_2 - INFO - Process 2 - Task 12/45 completed successfully
2025-06-07 08:42:02,903 - process_2 - INFO - Process 2 - Task 13/45 - seed: 31, exp: 10way_10shot, lambda: 0.4
2025-06-07 08:42:03,026 - process_2 - INFO - Process 2 - Starting training
2025-06-07 11:16:22,199 - process_2 - INFO - Process 2 - Starting testing
2025-06-07 11:35:15,007 - process_2 - INFO - Process 2 - Task 13/45 completed successfully
2025-06-07 11:35:15,008 - process_2 - INFO - Process 2 - Task 14/45 - seed: 31, exp: 10way_10shot, lambda: 0.6
2025-06-07 11:35:15,185 - process_2 - INFO - Process 2 - Starting training
2025-06-07 14:27:17,755 - process_2 - INFO - Process 2 - Starting testing
2025-06-07 14:44:39,575 - process_2 - INFO - Process 2 - Task 14/45 completed successfully
2025-06-07 14:44:39,575 - process_2 - INFO - Process 2 - Task 15/45 - seed: 31, exp: 10way_10shot, lambda: 0.7
2025-06-07 14:44:39,698 - process_2 - INFO - Process 2 - Starting training
2025-06-07 16:13:23,201 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_10shot, lambda=0.7): CUDA out of memory. Tried to allocate 3.47 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.38 GiB is free. Including non-PyTorch memory, this process has 11.14 GiB memory in use. Of the allocated memory 7.39 GiB is allocated by PyTorch, and 3.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 16:13:23,201 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.47 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.38 GiB is free. Including non-PyTorch memory, this process has 11.14 GiB memory in use. Of the allocated memory 7.39 GiB is allocated by PyTorch, and 3.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 16:13:23,452 - process_2 - INFO - Process 2 - Task 16/45 - seed: 31, exp: 10way_10shot, lambda: 0.8
2025-06-07 16:13:24,631 - process_2 - INFO - Process 2 - Starting training
2025-06-07 16:55:25,884 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_10shot, lambda=0.8): CUDA out of memory. Tried to allocate 3.72 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 11.18 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 16:55:25,885 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.72 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.34 GiB is free. Including non-PyTorch memory, this process has 11.18 GiB memory in use. Of the allocated memory 7.87 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 16:55:26,206 - process_2 - INFO - Process 2 - Task 17/45 - seed: 31, exp: 10way_10shot, lambda: 0.9
2025-06-07 16:55:27,312 - process_2 - INFO - Process 2 - Starting training
2025-06-07 20:04:08,511 - process_2 - INFO - Process 2 - Starting testing
2025-06-07 20:23:12,700 - process_2 - INFO - Process 2 - Task 17/45 completed successfully
2025-06-07 20:23:12,700 - process_2 - INFO - Process 2 - Task 18/45 - seed: 31, exp: 10way_10shot, lambda: 1.0
2025-06-07 20:23:12,826 - process_2 - INFO - Process 2 - Starting training
2025-06-07 22:35:32,816 - process_2 - ERROR - Process 2 error with task (seed=31, exp=10way_10shot, lambda=1.0): CUDA out of memory. Tried to allocate 3.78 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.15 GiB is free. Including non-PyTorch memory, this process has 11.37 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 3.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 22:35:32,816 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.15 GiB is free. Including non-PyTorch memory, this process has 11.37 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 3.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 22:35:33,088 - process_2 - INFO - Process 2 - Task 19/45 - seed: 42, exp: 5way_5shot, lambda: 0.1
2025-06-07 22:35:33,348 - process_2 - INFO - Process 2 - Starting training
2025-06-08 02:04:02,676 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 02:17:59,538 - process_2 - INFO - Process 2 - Task 19/45 completed successfully
2025-06-08 02:17:59,539 - process_2 - INFO - Process 2 - Task 20/45 - seed: 42, exp: 5way_5shot, lambda: 0.2
2025-06-08 02:17:59,696 - process_2 - INFO - Process 2 - Starting training
2025-06-08 04:04:10,684 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 04:18:10,904 - process_2 - INFO - Process 2 - Task 20/45 completed successfully
2025-06-08 04:18:10,904 - process_2 - INFO - Process 2 - Task 21/45 - seed: 42, exp: 5way_5shot, lambda: 0.3
2025-06-08 04:18:11,040 - process_2 - INFO - Process 2 - Starting training
2025-06-08 07:03:41,677 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 07:17:41,410 - process_2 - INFO - Process 2 - Task 21/45 completed successfully
2025-06-08 07:17:41,410 - process_2 - INFO - Process 2 - Task 22/45 - seed: 42, exp: 5way_5shot, lambda: 0.4
2025-06-08 07:17:41,541 - process_2 - INFO - Process 2 - Starting training
2025-06-08 08:52:14,067 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 09:06:08,369 - process_2 - INFO - Process 2 - Task 22/45 completed successfully
2025-06-08 09:06:08,371 - process_2 - INFO - Process 2 - Task 23/45 - seed: 42, exp: 5way_5shot, lambda: 0.6
2025-06-08 09:06:08,490 - process_2 - INFO - Process 2 - Starting training
2025-06-08 14:01:03,649 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 14:15:01,166 - process_2 - INFO - Process 2 - Task 23/45 completed successfully
2025-06-08 14:15:01,169 - process_2 - INFO - Process 2 - Task 24/45 - seed: 42, exp: 5way_5shot, lambda: 0.7
2025-06-08 14:15:01,316 - process_2 - INFO - Process 2 - Starting training
2025-06-08 17:09:25,075 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 17:23:22,328 - process_2 - INFO - Process 2 - Task 24/45 completed successfully
2025-06-08 17:23:22,328 - process_2 - INFO - Process 2 - Task 25/45 - seed: 42, exp: 5way_5shot, lambda: 0.8
2025-06-08 17:23:22,461 - process_2 - INFO - Process 2 - Starting training
2025-06-08 20:38:16,492 - process_2 - INFO - Process 2 - Starting testing
2025-06-08 20:52:10,860 - process_2 - INFO - Process 2 - Task 25/45 completed successfully
2025-06-08 20:52:10,862 - process_2 - INFO - Process 2 - Task 26/45 - seed: 42, exp: 5way_5shot, lambda: 0.9
2025-06-08 20:52:11,052 - process_2 - INFO - Process 2 - Starting training
2025-06-09 00:43:22,427 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 00:57:13,578 - process_2 - INFO - Process 2 - Task 26/45 completed successfully
2025-06-09 00:57:13,578 - process_2 - INFO - Process 2 - Task 27/45 - seed: 42, exp: 5way_5shot, lambda: 1.0
2025-06-09 00:57:13,746 - process_2 - INFO - Process 2 - Starting training
2025-06-09 04:00:58,719 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 04:14:51,556 - process_2 - INFO - Process 2 - Task 27/45 completed successfully
2025-06-09 04:14:51,556 - process_2 - INFO - Process 2 - Task 28/45 - seed: 42, exp: 5way_10shot, lambda: 0.1
2025-06-09 04:14:51,684 - process_2 - INFO - Process 2 - Starting training
2025-06-09 05:46:01,285 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 05:59:32,369 - process_2 - INFO - Process 2 - Task 28/45 completed successfully
2025-06-09 05:59:32,371 - process_2 - INFO - Process 2 - Task 29/45 - seed: 42, exp: 5way_10shot, lambda: 0.2
2025-06-09 05:59:32,490 - process_2 - INFO - Process 2 - Starting training
2025-06-09 06:16:41,370 - process_2 - ERROR - Process 2 error with task (seed=42, exp=5way_10shot, lambda=0.2): DataLoader worker (pid 4079276) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-09 06:16:41,370 - process_2 - ERROR - Process 2 - Exception details: RuntimeError: DataLoader worker (pid 4079276) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-09 06:16:41,727 - process_2 - INFO - Process 2 - Task 30/45 - seed: 42, exp: 5way_10shot, lambda: 0.3
2025-06-09 06:16:42,006 - process_2 - INFO - Process 2 - Starting training
2025-06-09 06:17:32,664 - process_2 - ERROR - Process 2 error with task (seed=42, exp=5way_10shot, lambda=0.3): DataLoader worker (pid(s) 4080565) exited unexpectedly
2025-06-09 06:17:32,664 - process_2 - ERROR - Process 2 - Exception details: RuntimeError: DataLoader worker (pid(s) 4080565) exited unexpectedly
2025-06-09 06:17:33,052 - process_2 - INFO - Process 2 - Task 31/45 - seed: 42, exp: 5way_10shot, lambda: 0.4
2025-06-09 06:17:33,324 - process_2 - INFO - Process 2 - Starting training
2025-06-09 06:17:44,197 - process_2 - ERROR - Process 2 error with task (seed=42, exp=5way_10shot, lambda=0.4): unable to write to file </torch_2399225_1918598860_400338>: No space left on device (28)
2025-06-09 06:17:44,197 - process_2 - ERROR - Process 2 - Exception details: RuntimeError: unable to write to file </torch_2399225_1918598860_400338>: No space left on device (28)
2025-06-09 06:17:44,454 - process_2 - INFO - Process 2 - Task 32/45 - seed: 42, exp: 5way_10shot, lambda: 0.6
2025-06-09 06:17:44,990 - process_2 - INFO - Process 2 - Starting training
2025-06-09 08:31:12,217 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 08:45:01,058 - process_2 - INFO - Process 2 - Task 32/45 completed successfully
2025-06-09 08:45:01,058 - process_2 - INFO - Process 2 - Task 33/45 - seed: 42, exp: 5way_10shot, lambda: 0.7
2025-06-09 08:45:01,173 - process_2 - INFO - Process 2 - Starting training
2025-06-09 11:15:03,310 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 11:28:45,246 - process_2 - INFO - Process 2 - Task 33/45 completed successfully
2025-06-09 11:28:45,248 - process_2 - INFO - Process 2 - Task 34/45 - seed: 42, exp: 5way_10shot, lambda: 0.8
2025-06-09 11:28:45,365 - process_2 - INFO - Process 2 - Starting training
2025-06-09 15:29:29,108 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 15:43:05,756 - process_2 - INFO - Process 2 - Task 34/45 completed successfully
2025-06-09 15:43:05,757 - process_2 - INFO - Process 2 - Task 35/45 - seed: 42, exp: 5way_10shot, lambda: 0.9
2025-06-09 15:43:05,885 - process_2 - INFO - Process 2 - Starting training
2025-06-09 17:55:57,421 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 18:10:05,121 - process_2 - INFO - Process 2 - Task 35/45 completed successfully
2025-06-09 18:10:05,121 - process_2 - INFO - Process 2 - Task 36/45 - seed: 42, exp: 5way_10shot, lambda: 1.0
2025-06-09 18:10:05,254 - process_2 - INFO - Process 2 - Starting training
2025-06-09 19:48:22,469 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 20:02:35,352 - process_2 - INFO - Process 2 - Task 36/45 completed successfully
2025-06-09 20:02:35,353 - process_2 - INFO - Process 2 - Task 37/45 - seed: 42, exp: 10way_5shot, lambda: 0.1
2025-06-09 20:02:35,468 - process_2 - INFO - Process 2 - Starting training
2025-06-09 23:34:13,530 - process_2 - INFO - Process 2 - Starting testing
2025-06-09 23:53:41,258 - process_2 - INFO - Process 2 - Task 37/45 completed successfully
2025-06-09 23:53:41,258 - process_2 - INFO - Process 2 - Task 38/45 - seed: 42, exp: 10way_5shot, lambda: 0.2
2025-06-09 23:53:41,435 - process_2 - INFO - Process 2 - Starting training
2025-06-10 00:11:23,691 - process_2 - ERROR - Process 2 error with task (seed=42, exp=10way_5shot, lambda=0.2): CUDA out of memory. Tried to allocate 3.50 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.04 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 00:11:23,691 - process_2 - ERROR - Process 2 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.50 GiB. GPU 2 has a total capacity of 14.53 GiB of which 3.04 GiB is free. Including non-PyTorch memory, this process has 11.48 GiB memory in use. Of the allocated memory 7.32 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 00:11:23,906 - process_2 - INFO - Process 2 - Task 39/45 - seed: 42, exp: 10way_5shot, lambda: 0.3
2025-06-10 00:11:24,137 - process_2 - INFO - Process 2 - Starting training
2025-06-10 04:27:46,561 - process_2 - INFO - Process 2 - Starting testing
2025-06-10 04:42:08,541 - process_2 - INFO - Process 2 - Task 39/45 completed successfully
2025-06-10 04:42:08,542 - process_2 - INFO - Process 2 - Task 40/45 - seed: 42, exp: 10way_5shot, lambda: 0.4
2025-06-10 04:42:08,652 - process_2 - INFO - Process 2 - Starting training
2025-06-10 08:22:05,482 - process_2 - INFO - Process 2 - Starting testing
2025-06-10 08:36:17,391 - process_2 - INFO - Process 2 - Task 40/45 completed successfully
2025-06-10 08:36:17,391 - process_2 - INFO - Process 2 - Task 41/45 - seed: 42, exp: 10way_5shot, lambda: 0.6
2025-06-10 08:36:17,515 - process_2 - INFO - Process 2 - Starting training
2025-06-10 11:04:29,717 - process_2 - INFO - Process 2 - Starting testing
2025-06-10 11:18:40,861 - process_2 - INFO - Process 2 - Task 41/45 completed successfully
2025-06-10 11:18:40,862 - process_2 - INFO - Process 2 - Task 42/45 - seed: 42, exp: 10way_5shot, lambda: 0.7
2025-06-10 11:18:40,971 - process_2 - INFO - Process 2 - Starting training
2025-06-10 13:55:15,698 - process_2 - INFO - Process 2 - Starting testing
2025-06-10 14:14:50,971 - process_2 - INFO - Process 2 - Task 42/45 completed successfully
2025-06-10 14:14:50,972 - process_2 - INFO - Process 2 - Task 43/45 - seed: 42, exp: 10way_5shot, lambda: 0.8
2025-06-10 14:14:51,090 - process_2 - INFO - Process 2 - Starting training
2025-06-10 17:48:17,078 - process_2 - INFO - Process 2 - Starting testing
2025-06-10 18:07:55,253 - process_2 - INFO - Process 2 - Task 43/45 completed successfully
2025-06-10 18:07:55,255 - process_2 - INFO - Process 2 - Task 44/45 - seed: 42, exp: 10way_5shot, lambda: 0.9
2025-06-10 18:07:55,375 - process_2 - INFO - Process 2 - Starting training
2025-06-10 20:13:52,436 - process_2 - ERROR - Process 2 error with task (seed=42, exp=10way_5shot, lambda=0.9): DataLoader worker (pid 1585190) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-10 20:13:52,437 - process_2 - ERROR - Process 2 - Exception details: RuntimeError: DataLoader worker (pid 1585190) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-10 20:13:53,133 - process_2 - INFO - Process 2 - Task 45/45 - seed: 42, exp: 10way_5shot, lambda: 1.0
2025-06-10 20:13:53,382 - process_2 - INFO - Process 2 - Starting training
2025-06-10 20:14:19,472 - process_2 - ERROR - Process 2 error with task (seed=42, exp=10way_5shot, lambda=1.0): DataLoader worker (pid(s) 1586764) exited unexpectedly
2025-06-10 20:14:19,472 - process_2 - ERROR - Process 2 - Exception details: RuntimeError: DataLoader worker (pid(s) 1586764) exited unexpectedly
2025-06-10 20:14:19,911 - process_2 - INFO - Process 2 completed all tasks
