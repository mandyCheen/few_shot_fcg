2025-06-05 17:06:41,479 - process_5 - INFO - Process 5 started on cuda:5
2025-06-05 17:06:41,591 - process_5 - INFO - Process 5 - Task 1/45 - seed: 10, exp: 5way_10shot, lambda: 0.1
2025-06-05 17:06:41,744 - process_5 - INFO - Process 5 - Loading dataset
2025-06-05 17:06:41,767 - process_5 - INFO - Process 5 - Starting training
2025-06-05 19:26:49,700 - process_5 - INFO - Process 5 - Starting testing
2025-06-05 19:43:03,384 - process_5 - INFO - Process 5 - Task 1/45 completed successfully
2025-06-05 19:43:03,386 - process_5 - INFO - Process 5 - Task 2/45 - seed: 10, exp: 5way_10shot, lambda: 0.2
2025-06-05 19:43:03,503 - process_5 - INFO - Process 5 - Starting training
2025-06-05 21:39:12,286 - process_5 - INFO - Process 5 - Starting testing
2025-06-05 21:56:26,794 - process_5 - INFO - Process 5 - Task 2/45 completed successfully
2025-06-05 21:56:26,795 - process_5 - INFO - Process 5 - Task 3/45 - seed: 10, exp: 5way_10shot, lambda: 0.3
2025-06-05 21:56:26,971 - process_5 - INFO - Process 5 - Starting training
2025-06-06 00:21:38,840 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 00:38:15,056 - process_5 - INFO - Process 5 - Task 3/45 completed successfully
2025-06-06 00:38:15,056 - process_5 - INFO - Process 5 - Task 4/45 - seed: 10, exp: 5way_10shot, lambda: 0.4
2025-06-06 00:38:15,210 - process_5 - INFO - Process 5 - Starting training
2025-06-06 04:30:12,844 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 04:47:01,454 - process_5 - INFO - Process 5 - Task 4/45 completed successfully
2025-06-06 04:47:01,458 - process_5 - INFO - Process 5 - Task 5/45 - seed: 10, exp: 5way_10shot, lambda: 0.6
2025-06-06 04:47:01,646 - process_5 - INFO - Process 5 - Starting training
2025-06-06 07:46:28,345 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 08:03:06,755 - process_5 - INFO - Process 5 - Task 5/45 completed successfully
2025-06-06 08:03:06,757 - process_5 - INFO - Process 5 - Task 6/45 - seed: 10, exp: 5way_10shot, lambda: 0.7
2025-06-06 08:03:06,900 - process_5 - INFO - Process 5 - Starting training
2025-06-06 10:07:34,951 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 10:24:40,633 - process_5 - INFO - Process 5 - Task 6/45 completed successfully
2025-06-06 10:24:40,635 - process_5 - INFO - Process 5 - Task 7/45 - seed: 10, exp: 5way_10shot, lambda: 0.8
2025-06-06 10:24:40,806 - process_5 - INFO - Process 5 - Starting training
2025-06-06 12:54:26,476 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 13:11:17,761 - process_5 - INFO - Process 5 - Task 7/45 completed successfully
2025-06-06 13:11:17,765 - process_5 - INFO - Process 5 - Task 8/45 - seed: 10, exp: 5way_10shot, lambda: 0.9
2025-06-06 13:11:17,984 - process_5 - INFO - Process 5 - Starting training
2025-06-06 15:54:47,410 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 16:11:22,526 - process_5 - INFO - Process 5 - Task 8/45 completed successfully
2025-06-06 16:11:22,527 - process_5 - INFO - Process 5 - Task 9/45 - seed: 10, exp: 5way_10shot, lambda: 1.0
2025-06-06 16:11:22,651 - process_5 - INFO - Process 5 - Starting training
2025-06-06 18:13:41,909 - process_5 - INFO - Process 5 - Starting testing
2025-06-06 18:30:47,077 - process_5 - INFO - Process 5 - Task 9/45 completed successfully
2025-06-06 18:30:47,078 - process_5 - INFO - Process 5 - Task 10/45 - seed: 10, exp: 10way_5shot, lambda: 0.1
2025-06-06 18:30:47,196 - process_5 - INFO - Process 5 - Starting training
2025-06-06 22:37:26,162 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_5shot, lambda=0.1): CUDA out of memory. Tried to allocate 3.75 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.72 GiB is free. Including non-PyTorch memory, this process has 10.80 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 22:37:26,162 - process_5 - ERROR - Process 5 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.75 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.72 GiB is free. Including non-PyTorch memory, this process has 10.80 GiB memory in use. Of the allocated memory 7.84 GiB is allocated by PyTorch, and 2.84 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-06 22:37:26,686 - process_5 - INFO - Process 5 - Task 11/45 - seed: 10, exp: 10way_5shot, lambda: 0.2
2025-06-06 22:37:27,071 - process_5 - INFO - Process 5 - Starting training
2025-06-07 02:12:45,420 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_5shot, lambda=0.2): CUDA out of memory. Tried to allocate 3.52 GiB. GPU 5 has a total capacity of 14.53 GiB of which 2.79 GiB is free. Including non-PyTorch memory, this process has 11.74 GiB memory in use. Of the allocated memory 7.23 GiB is allocated by PyTorch, and 4.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 02:12:45,421 - process_5 - ERROR - Process 5 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.52 GiB. GPU 5 has a total capacity of 14.53 GiB of which 2.79 GiB is free. Including non-PyTorch memory, this process has 11.74 GiB memory in use. Of the allocated memory 7.23 GiB is allocated by PyTorch, and 4.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 02:12:45,816 - process_5 - INFO - Process 5 - Task 12/45 - seed: 10, exp: 10way_5shot, lambda: 0.3
2025-06-07 02:12:46,182 - process_5 - INFO - Process 5 - Starting training
2025-06-07 05:25:39,011 - process_5 - INFO - Process 5 - Starting testing
2025-06-07 05:48:23,139 - process_5 - INFO - Process 5 - Task 12/45 completed successfully
2025-06-07 05:48:23,141 - process_5 - INFO - Process 5 - Task 13/45 - seed: 10, exp: 10way_5shot, lambda: 0.4
2025-06-07 05:48:23,269 - process_5 - INFO - Process 5 - Starting training
2025-06-07 06:34:15,685 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_5shot, lambda=0.4): CUDA out of memory. Tried to allocate 3.81 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.54 GiB is free. Including non-PyTorch memory, this process has 10.98 GiB memory in use. Of the allocated memory 7.73 GiB is allocated by PyTorch, and 3.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 06:34:15,686 - process_5 - ERROR - Process 5 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.81 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.54 GiB is free. Including non-PyTorch memory, this process has 10.98 GiB memory in use. Of the allocated memory 7.73 GiB is allocated by PyTorch, and 3.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-07 06:34:16,015 - process_5 - INFO - Process 5 - Task 14/45 - seed: 10, exp: 10way_5shot, lambda: 0.6
2025-06-07 06:34:16,658 - process_5 - INFO - Process 5 - Starting training
2025-06-07 09:40:24,384 - process_5 - INFO - Process 5 - Starting testing
2025-06-07 10:03:12,294 - process_5 - INFO - Process 5 - Task 14/45 completed successfully
2025-06-07 10:03:12,295 - process_5 - INFO - Process 5 - Task 15/45 - seed: 10, exp: 10way_5shot, lambda: 0.7
2025-06-07 10:03:12,416 - process_5 - INFO - Process 5 - Starting training
2025-06-07 14:12:47,740 - process_5 - INFO - Process 5 - Starting testing
2025-06-07 14:35:32,429 - process_5 - INFO - Process 5 - Task 15/45 completed successfully
2025-06-07 14:35:32,430 - process_5 - INFO - Process 5 - Task 16/45 - seed: 10, exp: 10way_5shot, lambda: 0.8
2025-06-07 14:35:32,604 - process_5 - INFO - Process 5 - Starting training
2025-06-07 17:08:44,496 - process_5 - INFO - Process 5 - Starting testing
2025-06-07 17:31:38,502 - process_5 - INFO - Process 5 - Task 16/45 completed successfully
2025-06-07 17:31:38,504 - process_5 - INFO - Process 5 - Task 17/45 - seed: 10, exp: 10way_5shot, lambda: 0.9
2025-06-07 17:31:38,613 - process_5 - INFO - Process 5 - Starting training
2025-06-07 20:55:54,549 - process_5 - INFO - Process 5 - Starting testing
2025-06-07 21:18:24,994 - process_5 - INFO - Process 5 - Task 17/45 completed successfully
2025-06-07 21:18:24,996 - process_5 - INFO - Process 5 - Task 18/45 - seed: 10, exp: 10way_5shot, lambda: 1.0
2025-06-07 21:18:25,113 - process_5 - INFO - Process 5 - Starting training
2025-06-08 00:05:13,625 - process_5 - INFO - Process 5 - Starting testing
2025-06-08 00:27:39,691 - process_5 - INFO - Process 5 - Task 18/45 completed successfully
2025-06-08 00:27:39,692 - process_5 - INFO - Process 5 - Task 19/45 - seed: 10, exp: 10way_10shot, lambda: 0.1
2025-06-08 00:27:39,817 - process_5 - INFO - Process 5 - Starting training
2025-06-08 00:38:06,877 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_10shot, lambda=0.1): CUDA out of memory. Tried to allocate 4.08 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.42 GiB is free. Including non-PyTorch memory, this process has 11.10 GiB memory in use. Of the allocated memory 8.46 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 00:38:06,877 - process_5 - ERROR - Process 5 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 4.08 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.42 GiB is free. Including non-PyTorch memory, this process has 11.10 GiB memory in use. Of the allocated memory 8.46 GiB is allocated by PyTorch, and 2.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 00:38:07,094 - process_5 - INFO - Process 5 - Task 20/45 - seed: 10, exp: 10way_10shot, lambda: 0.2
2025-06-08 00:38:07,313 - process_5 - INFO - Process 5 - Starting training
2025-06-08 04:38:58,730 - process_5 - INFO - Process 5 - Starting testing
2025-06-08 05:01:16,862 - process_5 - INFO - Process 5 - Task 20/45 completed successfully
2025-06-08 05:01:16,863 - process_5 - INFO - Process 5 - Task 21/45 - seed: 10, exp: 10way_10shot, lambda: 0.3
2025-06-08 05:01:17,034 - process_5 - INFO - Process 5 - Starting training
2025-06-08 06:37:59,758 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_10shot, lambda=0.3): CUDA out of memory. Tried to allocate 3.95 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.25 GiB is free. Including non-PyTorch memory, this process has 11.27 GiB memory in use. Of the allocated memory 8.06 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 06:37:59,758 - process_5 - ERROR - Process 5 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.95 GiB. GPU 5 has a total capacity of 14.53 GiB of which 3.25 GiB is free. Including non-PyTorch memory, this process has 11.27 GiB memory in use. Of the allocated memory 8.06 GiB is allocated by PyTorch, and 3.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 06:38:00,031 - process_5 - INFO - Process 5 - Task 22/45 - seed: 10, exp: 10way_10shot, lambda: 0.4
2025-06-08 06:38:00,283 - process_5 - INFO - Process 5 - Starting training
2025-06-08 09:49:30,011 - process_5 - INFO - Process 5 - Starting testing
2025-06-08 10:11:56,942 - process_5 - INFO - Process 5 - Task 22/45 completed successfully
2025-06-08 10:11:56,944 - process_5 - INFO - Process 5 - Task 23/45 - seed: 10, exp: 10way_10shot, lambda: 0.6
2025-06-08 10:11:57,063 - process_5 - INFO - Process 5 - Starting training
2025-06-08 14:07:00,625 - process_5 - INFO - Process 5 - Starting testing
2025-06-08 14:29:16,592 - process_5 - INFO - Process 5 - Task 23/45 completed successfully
2025-06-08 14:29:16,594 - process_5 - INFO - Process 5 - Task 24/45 - seed: 10, exp: 10way_10shot, lambda: 0.7
2025-06-08 14:29:16,711 - process_5 - INFO - Process 5 - Starting training
2025-06-08 20:09:56,087 - process_5 - INFO - Process 5 - Starting testing
2025-06-08 20:32:14,041 - process_5 - INFO - Process 5 - Task 24/45 completed successfully
2025-06-08 20:32:14,043 - process_5 - INFO - Process 5 - Task 25/45 - seed: 10, exp: 10way_10shot, lambda: 0.8
2025-06-08 20:32:14,221 - process_5 - INFO - Process 5 - Starting training
2025-06-08 21:31:13,701 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_10shot, lambda=0.8): CUDA out of memory. Tried to allocate 3.45 GiB. GPU 5 has a total capacity of 14.53 GiB of which 2.93 GiB is free. Including non-PyTorch memory, this process has 11.59 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 4.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 21:31:13,701 - process_5 - ERROR - Process 5 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.45 GiB. GPU 5 has a total capacity of 14.53 GiB of which 2.93 GiB is free. Including non-PyTorch memory, this process has 11.59 GiB memory in use. Of the allocated memory 7.27 GiB is allocated by PyTorch, and 4.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 21:31:13,955 - process_5 - INFO - Process 5 - Task 26/45 - seed: 10, exp: 10way_10shot, lambda: 0.9
2025-06-08 21:31:15,117 - process_5 - INFO - Process 5 - Starting training
2025-06-09 02:55:16,668 - process_5 - INFO - Process 5 - Starting testing
2025-06-09 03:17:38,280 - process_5 - INFO - Process 5 - Task 26/45 completed successfully
2025-06-09 03:17:38,284 - process_5 - INFO - Process 5 - Task 27/45 - seed: 10, exp: 10way_10shot, lambda: 1.0
2025-06-09 03:17:38,414 - process_5 - INFO - Process 5 - Starting training
2025-06-09 06:16:41,707 - process_5 - ERROR - Process 5 error with task (seed=10, exp=10way_10shot, lambda=1.0): DataLoader worker (pid 4078682) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-09 06:16:41,707 - process_5 - ERROR - Process 5 - Exception details: RuntimeError: DataLoader worker (pid 4078682) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-09 06:16:41,967 - process_5 - INFO - Process 5 - Task 28/45 - seed: 666, exp: 5way_5shot, lambda: 0.1
2025-06-09 06:16:42,212 - process_5 - INFO - Process 5 - Starting training
2025-06-09 06:17:32,642 - process_5 - ERROR - Process 5 error with task (seed=666, exp=5way_5shot, lambda=0.1): DataLoader worker (pid(s) 4080174) exited unexpectedly
2025-06-09 06:17:32,643 - process_5 - ERROR - Process 5 - Exception details: RuntimeError: DataLoader worker (pid(s) 4080174) exited unexpectedly
2025-06-09 06:17:33,603 - process_5 - INFO - Process 5 - Task 29/45 - seed: 666, exp: 5way_5shot, lambda: 0.2
2025-06-09 06:17:33,813 - process_5 - INFO - Process 5 - Starting training
