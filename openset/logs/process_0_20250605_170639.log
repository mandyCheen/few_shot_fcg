2025-06-05 17:06:39,097 - process_0 - INFO - Process 0 started on cuda:0
2025-06-05 17:06:40,770 - process_0 - INFO - Process 0 - Task 1/45 - seed: 6, exp: 5way_5shot, lambda: 0.1
2025-06-05 17:06:40,974 - process_0 - INFO - Process 0 - Loading dataset
2025-06-05 17:06:40,996 - process_0 - INFO - Process 0 - Starting training
2025-06-05 19:47:06,370 - process_0 - INFO - Process 0 - Starting testing
2025-06-05 20:01:46,913 - process_0 - INFO - Process 0 - Task 1/45 completed successfully
2025-06-05 20:01:46,916 - process_0 - INFO - Process 0 - Task 2/45 - seed: 6, exp: 5way_5shot, lambda: 0.2
2025-06-05 20:01:47,024 - process_0 - INFO - Process 0 - Starting training
2025-06-06 00:24:15,853 - process_0 - INFO - Process 0 - Starting testing
2025-06-06 00:38:41,988 - process_0 - INFO - Process 0 - Task 2/45 completed successfully
2025-06-06 00:38:41,990 - process_0 - INFO - Process 0 - Task 3/45 - seed: 6, exp: 5way_5shot, lambda: 0.3
2025-06-06 00:38:42,161 - process_0 - INFO - Process 0 - Starting training
2025-06-06 02:54:28,826 - process_0 - INFO - Process 0 - Starting testing
2025-06-06 03:09:18,271 - process_0 - INFO - Process 0 - Task 3/45 completed successfully
2025-06-06 03:09:18,273 - process_0 - INFO - Process 0 - Task 4/45 - seed: 6, exp: 5way_5shot, lambda: 0.4
2025-06-06 03:09:18,409 - process_0 - INFO - Process 0 - Starting training
2025-06-06 08:01:53,920 - process_0 - INFO - Process 0 - Starting testing
2025-06-06 08:16:40,109 - process_0 - INFO - Process 0 - Task 4/45 completed successfully
2025-06-06 08:16:40,110 - process_0 - INFO - Process 0 - Task 5/45 - seed: 6, exp: 5way_5shot, lambda: 0.6
2025-06-06 08:16:40,273 - process_0 - INFO - Process 0 - Starting training
2025-06-06 13:53:53,908 - process_0 - INFO - Process 0 - Starting testing
2025-06-06 14:08:30,894 - process_0 - INFO - Process 0 - Task 5/45 completed successfully
2025-06-06 14:08:30,894 - process_0 - INFO - Process 0 - Task 6/45 - seed: 6, exp: 5way_5shot, lambda: 0.7
2025-06-06 14:08:31,074 - process_0 - INFO - Process 0 - Starting training
2025-06-06 17:23:03,122 - process_0 - INFO - Process 0 - Starting testing
2025-06-06 17:37:50,380 - process_0 - INFO - Process 0 - Task 6/45 completed successfully
2025-06-06 17:37:50,382 - process_0 - INFO - Process 0 - Task 7/45 - seed: 6, exp: 5way_5shot, lambda: 0.8
2025-06-06 17:37:50,500 - process_0 - INFO - Process 0 - Starting training
2025-06-06 22:23:16,362 - process_0 - INFO - Process 0 - Starting testing
2025-06-06 22:38:06,371 - process_0 - INFO - Process 0 - Task 7/45 completed successfully
2025-06-06 22:38:06,374 - process_0 - INFO - Process 0 - Task 8/45 - seed: 6, exp: 5way_5shot, lambda: 0.9
2025-06-06 22:38:06,493 - process_0 - INFO - Process 0 - Starting training
2025-06-07 02:27:16,788 - process_0 - INFO - Process 0 - Starting testing
2025-06-07 02:42:04,043 - process_0 - INFO - Process 0 - Task 8/45 completed successfully
2025-06-07 02:42:04,043 - process_0 - INFO - Process 0 - Task 9/45 - seed: 6, exp: 5way_5shot, lambda: 1.0
2025-06-07 02:42:04,206 - process_0 - INFO - Process 0 - Starting training
2025-06-07 07:32:23,812 - process_0 - INFO - Process 0 - Starting testing
2025-06-07 07:47:07,325 - process_0 - INFO - Process 0 - Task 9/45 completed successfully
2025-06-07 07:47:07,327 - process_0 - INFO - Process 0 - Task 10/45 - seed: 6, exp: 5way_10shot, lambda: 0.1
2025-06-07 07:47:07,444 - process_0 - INFO - Process 0 - Starting training
2025-06-07 13:29:02,887 - process_0 - INFO - Process 0 - Starting testing
2025-06-07 13:43:05,727 - process_0 - INFO - Process 0 - Task 10/45 completed successfully
2025-06-07 13:43:05,728 - process_0 - INFO - Process 0 - Task 11/45 - seed: 6, exp: 5way_10shot, lambda: 0.2
2025-06-07 13:43:05,845 - process_0 - INFO - Process 0 - Starting training
2025-06-07 16:08:34,752 - process_0 - INFO - Process 0 - Starting testing
2025-06-07 16:23:01,354 - process_0 - INFO - Process 0 - Task 11/45 completed successfully
2025-06-07 16:23:01,356 - process_0 - INFO - Process 0 - Task 12/45 - seed: 6, exp: 5way_10shot, lambda: 0.3
2025-06-07 16:23:01,491 - process_0 - INFO - Process 0 - Starting training
2025-06-07 18:42:15,275 - process_0 - INFO - Process 0 - Starting testing
2025-06-07 18:56:33,301 - process_0 - INFO - Process 0 - Task 12/45 completed successfully
2025-06-07 18:56:33,301 - process_0 - INFO - Process 0 - Task 13/45 - seed: 6, exp: 5way_10shot, lambda: 0.4
2025-06-07 18:56:33,429 - process_0 - INFO - Process 0 - Starting training
2025-06-08 00:09:19,585 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 00:23:37,255 - process_0 - INFO - Process 0 - Task 13/45 completed successfully
2025-06-08 00:23:37,257 - process_0 - INFO - Process 0 - Task 14/45 - seed: 6, exp: 5way_10shot, lambda: 0.6
2025-06-08 00:23:37,396 - process_0 - INFO - Process 0 - Starting training
2025-06-08 02:00:42,630 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 02:14:47,501 - process_0 - INFO - Process 0 - Task 14/45 completed successfully
2025-06-08 02:14:47,505 - process_0 - INFO - Process 0 - Task 15/45 - seed: 6, exp: 5way_10shot, lambda: 0.7
2025-06-08 02:14:47,622 - process_0 - INFO - Process 0 - Starting training
2025-06-08 04:09:17,277 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 04:23:49,205 - process_0 - INFO - Process 0 - Task 15/45 completed successfully
2025-06-08 04:23:49,207 - process_0 - INFO - Process 0 - Task 16/45 - seed: 6, exp: 5way_10shot, lambda: 0.8
2025-06-08 04:23:49,392 - process_0 - INFO - Process 0 - Starting training
2025-06-08 09:17:41,697 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 09:32:10,923 - process_0 - INFO - Process 0 - Task 16/45 completed successfully
2025-06-08 09:32:10,923 - process_0 - INFO - Process 0 - Task 17/45 - seed: 6, exp: 5way_10shot, lambda: 0.9
2025-06-08 09:32:11,041 - process_0 - INFO - Process 0 - Starting training
2025-06-08 11:47:02,367 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 12:01:04,699 - process_0 - INFO - Process 0 - Task 17/45 completed successfully
2025-06-08 12:01:04,699 - process_0 - INFO - Process 0 - Task 18/45 - seed: 6, exp: 5way_10shot, lambda: 1.0
2025-06-08 12:01:04,815 - process_0 - INFO - Process 0 - Starting training
2025-06-08 17:11:54,358 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 17:26:21,132 - process_0 - INFO - Process 0 - Task 18/45 completed successfully
2025-06-08 17:26:21,133 - process_0 - INFO - Process 0 - Task 19/45 - seed: 6, exp: 10way_5shot, lambda: 0.1
2025-06-08 17:26:21,266 - process_0 - INFO - Process 0 - Starting training
2025-06-08 18:12:29,823 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_5shot, lambda=0.1): CUDA out of memory. Tried to allocate 3.69 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.64 GiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 7.61 GiB is allocated by PyTorch, and 3.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 18:12:29,823 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.69 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.64 GiB is free. Including non-PyTorch memory, this process has 10.88 GiB memory in use. Of the allocated memory 7.61 GiB is allocated by PyTorch, and 3.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-08 18:12:30,118 - process_0 - INFO - Process 0 - Task 20/45 - seed: 6, exp: 10way_5shot, lambda: 0.2
2025-06-08 18:12:31,558 - process_0 - INFO - Process 0 - Starting training
2025-06-08 20:45:37,584 - process_0 - INFO - Process 0 - Starting testing
2025-06-08 21:04:06,068 - process_0 - INFO - Process 0 - Task 20/45 completed successfully
2025-06-08 21:04:06,069 - process_0 - INFO - Process 0 - Task 21/45 - seed: 6, exp: 10way_5shot, lambda: 0.3
2025-06-08 21:04:06,180 - process_0 - INFO - Process 0 - Starting training
2025-06-09 00:01:53,603 - process_0 - INFO - Process 0 - Starting testing
2025-06-09 00:20:26,459 - process_0 - INFO - Process 0 - Task 21/45 completed successfully
2025-06-09 00:20:26,462 - process_0 - INFO - Process 0 - Task 22/45 - seed: 6, exp: 10way_5shot, lambda: 0.4
2025-06-09 00:20:26,655 - process_0 - INFO - Process 0 - Starting training
2025-06-09 05:50:58,629 - process_0 - INFO - Process 0 - Starting testing
2025-06-09 06:09:44,603 - process_0 - INFO - Process 0 - Task 22/45 completed successfully
2025-06-09 06:09:44,608 - process_0 - INFO - Process 0 - Task 23/45 - seed: 6, exp: 10way_5shot, lambda: 0.6
2025-06-09 06:09:44,727 - process_0 - INFO - Process 0 - Starting training
2025-06-09 10:48:50,883 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_5shot, lambda=0.6): CUDA out of memory. Tried to allocate 3.76 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.65 GiB is free. Including non-PyTorch memory, this process has 10.87 GiB memory in use. Of the allocated memory 7.72 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 10:48:50,883 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.76 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.65 GiB is free. Including non-PyTorch memory, this process has 10.87 GiB memory in use. Of the allocated memory 7.72 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 10:48:51,215 - process_0 - INFO - Process 0 - Task 24/45 - seed: 6, exp: 10way_5shot, lambda: 0.7
2025-06-09 10:48:51,557 - process_0 - INFO - Process 0 - Starting training
2025-06-09 11:02:32,022 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_5shot, lambda=0.7): CUDA out of memory. Tried to allocate 3.82 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.76 GiB is free. Including non-PyTorch memory, this process has 10.76 GiB memory in use. Of the allocated memory 7.91 GiB is allocated by PyTorch, and 2.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 11:02:32,022 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.82 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.76 GiB is free. Including non-PyTorch memory, this process has 10.76 GiB memory in use. Of the allocated memory 7.91 GiB is allocated by PyTorch, and 2.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 11:02:32,292 - process_0 - INFO - Process 0 - Task 25/45 - seed: 6, exp: 10way_5shot, lambda: 0.8
2025-06-09 11:02:32,843 - process_0 - INFO - Process 0 - Starting training
2025-06-09 15:40:14,256 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_5shot, lambda=0.8): CUDA out of memory. Tried to allocate 3.19 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.04 GiB is free. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 6.68 GiB is allocated by PyTorch, and 4.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:40:14,257 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.19 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.04 GiB is free. Including non-PyTorch memory, this process has 11.49 GiB memory in use. Of the allocated memory 6.68 GiB is allocated by PyTorch, and 4.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 15:40:14,500 - process_0 - INFO - Process 0 - Task 26/45 - seed: 6, exp: 10way_5shot, lambda: 0.9
2025-06-09 15:40:15,808 - process_0 - INFO - Process 0 - Starting training
2025-06-09 17:55:27,640 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_5shot, lambda=0.9): CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.92 GiB is free. Including non-PyTorch memory, this process has 11.60 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 4.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 17:55:27,641 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.38 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.92 GiB is free. Including non-PyTorch memory, this process has 11.60 GiB memory in use. Of the allocated memory 7.04 GiB is allocated by PyTorch, and 4.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 17:55:27,993 - process_0 - INFO - Process 0 - Task 27/45 - seed: 6, exp: 10way_5shot, lambda: 1.0
2025-06-09 17:55:28,566 - process_0 - INFO - Process 0 - Starting training
2025-06-09 19:34:15,302 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_5shot, lambda=1.0): CUDA out of memory. Tried to allocate 3.43 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.13 GiB is free. Including non-PyTorch memory, this process has 11.39 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 4.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 19:34:15,303 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.43 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.13 GiB is free. Including non-PyTorch memory, this process has 11.39 GiB memory in use. Of the allocated memory 7.03 GiB is allocated by PyTorch, and 4.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 19:34:15,725 - process_0 - INFO - Process 0 - Task 28/45 - seed: 6, exp: 10way_10shot, lambda: 0.1
2025-06-09 19:34:16,136 - process_0 - INFO - Process 0 - Starting training
2025-06-09 22:21:48,807 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_10shot, lambda=0.1): CUDA out of memory. Tried to allocate 3.96 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 11.90 GiB memory in use. Of the allocated memory 8.28 GiB is allocated by PyTorch, and 3.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 22:21:48,808 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.96 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 11.90 GiB memory in use. Of the allocated memory 8.28 GiB is allocated by PyTorch, and 3.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-09 22:21:49,030 - process_0 - INFO - Process 0 - Task 29/45 - seed: 6, exp: 10way_10shot, lambda: 0.2
2025-06-09 22:21:49,413 - process_0 - INFO - Process 0 - Starting training
2025-06-10 04:31:11,008 - process_0 - INFO - Process 0 - Starting testing
2025-06-10 04:42:49,998 - process_0 - INFO - Process 0 - Task 29/45 completed successfully
2025-06-10 04:42:50,000 - process_0 - INFO - Process 0 - Task 30/45 - seed: 6, exp: 10way_10shot, lambda: 0.3
2025-06-10 04:42:50,111 - process_0 - INFO - Process 0 - Starting training
2025-06-10 07:11:25,909 - process_0 - INFO - Process 0 - Starting testing
2025-06-10 07:23:23,616 - process_0 - INFO - Process 0 - Task 30/45 completed successfully
2025-06-10 07:23:23,618 - process_0 - INFO - Process 0 - Task 31/45 - seed: 6, exp: 10way_10shot, lambda: 0.4
2025-06-10 07:23:23,776 - process_0 - INFO - Process 0 - Starting training
2025-06-10 08:25:48,197 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_10shot, lambda=0.4): CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.59 GiB is free. Including non-PyTorch memory, this process has 10.93 GiB memory in use. Of the allocated memory 7.85 GiB is allocated by PyTorch, and 2.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 08:25:48,197 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.80 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.59 GiB is free. Including non-PyTorch memory, this process has 10.93 GiB memory in use. Of the allocated memory 7.85 GiB is allocated by PyTorch, and 2.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 08:25:48,438 - process_0 - INFO - Process 0 - Task 32/45 - seed: 6, exp: 10way_10shot, lambda: 0.6
2025-06-10 08:25:48,681 - process_0 - INFO - Process 0 - Starting training
2025-06-10 08:30:28,019 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_10shot, lambda=0.6): CUDA out of memory. Tried to allocate 3.28 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.99 GiB is free. Including non-PyTorch memory, this process has 11.54 GiB memory in use. Of the allocated memory 6.71 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 08:30:28,019 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.28 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.99 GiB is free. Including non-PyTorch memory, this process has 11.54 GiB memory in use. Of the allocated memory 6.71 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 08:30:28,184 - process_0 - INFO - Process 0 - Task 33/45 - seed: 6, exp: 10way_10shot, lambda: 0.7
2025-06-10 08:30:28,431 - process_0 - INFO - Process 0 - Starting training
2025-06-10 11:15:35,292 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_10shot, lambda=0.7): CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.02 GiB is free. Including non-PyTorch memory, this process has 11.50 GiB memory in use. Of the allocated memory 7.81 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 11:15:35,292 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.78 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.02 GiB is free. Including non-PyTorch memory, this process has 11.50 GiB memory in use. Of the allocated memory 7.81 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 11:15:35,474 - process_0 - INFO - Process 0 - Task 34/45 - seed: 6, exp: 10way_10shot, lambda: 0.8
2025-06-10 11:15:35,732 - process_0 - INFO - Process 0 - Starting training
2025-06-10 14:18:43,901 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_10shot, lambda=0.8): CUDA out of memory. Tried to allocate 3.45 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 11.90 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 4.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 14:18:43,901 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.45 GiB. GPU 0 has a total capacity of 14.53 GiB of which 2.62 GiB is free. Including non-PyTorch memory, this process has 11.90 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 4.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 14:18:44,224 - process_0 - INFO - Process 0 - Task 35/45 - seed: 6, exp: 10way_10shot, lambda: 0.9
2025-06-10 14:18:45,574 - process_0 - INFO - Process 0 - Starting training
2025-06-10 18:18:32,201 - process_0 - INFO - Process 0 - Starting testing
2025-06-10 18:37:50,105 - process_0 - INFO - Process 0 - Task 35/45 completed successfully
2025-06-10 18:37:50,105 - process_0 - INFO - Process 0 - Task 36/45 - seed: 6, exp: 10way_10shot, lambda: 1.0
2025-06-10 18:37:50,222 - process_0 - INFO - Process 0 - Starting training
2025-06-10 18:52:11,717 - process_0 - ERROR - Process 0 error with task (seed=6, exp=10way_10shot, lambda=1.0): CUDA out of memory. Tried to allocate 3.56 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process has 11.03 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 3.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 18:52:11,718 - process_0 - ERROR - Process 0 - Exception details: OutOfMemoryError: CUDA out of memory. Tried to allocate 3.56 GiB. GPU 0 has a total capacity of 14.53 GiB of which 3.49 GiB is free. Including non-PyTorch memory, this process has 11.03 GiB memory in use. Of the allocated memory 7.05 GiB is allocated by PyTorch, and 3.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-06-10 18:52:11,943 - process_0 - INFO - Process 0 - Task 37/45 - seed: 22, exp: 5way_5shot, lambda: 0.1
2025-06-10 18:52:12,444 - process_0 - INFO - Process 0 - Starting training
2025-06-10 20:13:18,630 - process_0 - ERROR - Process 0 error with task (seed=22, exp=5way_5shot, lambda=0.1): DataLoader worker (pid 1585728) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-10 20:13:18,630 - process_0 - ERROR - Process 0 - Exception details: RuntimeError: DataLoader worker (pid 1585728) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
2025-06-10 20:13:19,138 - process_0 - INFO - Process 0 - Task 38/45 - seed: 22, exp: 5way_5shot, lambda: 0.2
2025-06-10 20:13:19,384 - process_0 - INFO - Process 0 - Starting training
2025-06-11 03:18:31,514 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 03:33:31,538 - process_0 - INFO - Process 0 - Task 38/45 completed successfully
2025-06-11 03:33:31,538 - process_0 - INFO - Process 0 - Task 39/45 - seed: 22, exp: 5way_5shot, lambda: 0.3
2025-06-11 03:33:31,721 - process_0 - INFO - Process 0 - Starting training
2025-06-11 06:09:45,974 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 06:24:51,810 - process_0 - INFO - Process 0 - Task 39/45 completed successfully
2025-06-11 06:24:51,811 - process_0 - INFO - Process 0 - Task 40/45 - seed: 22, exp: 5way_5shot, lambda: 0.4
2025-06-11 06:24:51,930 - process_0 - INFO - Process 0 - Starting training
2025-06-11 08:18:21,746 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 08:33:36,024 - process_0 - INFO - Process 0 - Task 40/45 completed successfully
2025-06-11 08:33:36,024 - process_0 - INFO - Process 0 - Task 41/45 - seed: 22, exp: 5way_5shot, lambda: 0.6
2025-06-11 08:33:36,154 - process_0 - INFO - Process 0 - Starting training
2025-06-11 10:47:29,774 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 11:02:52,836 - process_0 - INFO - Process 0 - Task 41/45 completed successfully
2025-06-11 11:02:52,839 - process_0 - INFO - Process 0 - Task 42/45 - seed: 22, exp: 5way_5shot, lambda: 0.7
2025-06-11 11:02:53,006 - process_0 - INFO - Process 0 - Starting training
2025-06-11 16:50:10,486 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 17:04:34,249 - process_0 - INFO - Process 0 - Task 42/45 completed successfully
2025-06-11 17:04:34,250 - process_0 - INFO - Process 0 - Task 43/45 - seed: 22, exp: 5way_5shot, lambda: 0.8
2025-06-11 17:04:34,368 - process_0 - INFO - Process 0 - Starting training
2025-06-11 19:32:25,611 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 19:41:11,627 - process_0 - INFO - Process 0 - Task 43/45 completed successfully
2025-06-11 19:41:11,627 - process_0 - INFO - Process 0 - Task 44/45 - seed: 22, exp: 5way_5shot, lambda: 0.9
2025-06-11 19:41:11,734 - process_0 - INFO - Process 0 - Starting training
2025-06-11 22:32:15,610 - process_0 - INFO - Process 0 - Starting testing
2025-06-11 22:41:10,684 - process_0 - INFO - Process 0 - Task 44/45 completed successfully
2025-06-11 22:41:10,685 - process_0 - INFO - Process 0 - Task 45/45 - seed: 22, exp: 5way_5shot, lambda: 1.0
2025-06-11 22:41:10,801 - process_0 - INFO - Process 0 - Starting training
2025-06-12 00:37:25,881 - process_0 - INFO - Process 0 - Starting testing
2025-06-12 00:46:19,888 - process_0 - INFO - Process 0 - Task 45/45 completed successfully
2025-06-12 00:46:19,993 - process_0 - INFO - Process 0 completed all tasks
