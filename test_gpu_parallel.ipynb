{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'torch25 (Python -1.-1.-1)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1000, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def test_gpu_parallel():\n",
    "    # Check CUDA availability\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. Please check your PyTorch installation.\")\n",
    "        return\n",
    "\n",
    "    # Print GPU information\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {gpu_count}\")\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Create model and move to GPU\n",
    "    model = SimpleNet()\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "    print(\"\\nModel moved to GPU and wrapped in DataParallel\")\n",
    "\n",
    "    # Create sample input data\n",
    "    batch_size = 256\n",
    "    input_data = torch.randn(batch_size, 1000).cuda()\n",
    "    target = torch.randn(batch_size, 10).cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Test forward and backward pass\n",
    "    print(\"\\nTesting forward and backward pass...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run for a few iterations to test\n",
    "    for i in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nTime taken for 10 iterations: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Verify GPU memory usage\n",
    "    print(\"\\nGPU Memory Usage:\")\n",
    "    for i in range(gpu_count):\n",
    "        memory_allocated = torch.cuda.memory_allocated(i) / 1024**2\n",
    "        memory_cached = torch.cuda.memory_reserved(i) / 1024**2\n",
    "        print(f\"GPU {i}:\")\n",
    "        print(f\"  Allocated: {memory_allocated:.2f} MB\")\n",
    "        print(f\"  Cached:    {memory_cached:.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_gpu_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 8\n",
      "GPU 0: NVIDIA A16\n",
      "GPU 1: NVIDIA A16\n",
      "GPU 2: NVIDIA A16\n",
      "GPU 3: NVIDIA A16\n",
      "GPU 4: NVIDIA A16\n",
      "GPU 5: NVIDIA A16\n",
      "GPU 6: NVIDIA A16\n",
      "GPU 7: NVIDIA A16\n",
      "Current CUDA device: 0\n",
      "\n",
      "Creating model...\n",
      "Model created\n",
      "Model on CPU\n",
      "Creating input data...\n",
      "Input data created\n",
      "Moving model to CUDA...\n",
      "Model on CUDA\n",
      "Wrapping in DataParallel...\n",
      "Model wrapped in DataParallel\n",
      "Moving data to CUDA...\n",
      "Data moved to CUDA\n",
      "\n",
      "Testing forward and backward pass...\n",
      "Starting iteration 0\n",
      "Forward pass 0\n",
      "frostrend:1297655:1297655 [0] NCCL INFO cudaDriverVersion 12060\n",
      "frostrend:1297655:1297655 [0] NCCL INFO Bootstrap : Using ens97f0:10.10.133.215<0>\n",
      "frostrend:1297655:1297655 [0] NCCL INFO NET/Plugin : dlerror=libnccl-net.so: cannot open shared object file: No such file or directory No plugin found (libnccl-net.so), using internal implementation\n",
      "NCCL version 2.20.5+cuda11.8\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Failed to open libibverbs.so[.1]\n",
      "frostrend:1297655:1297712 [1] NCCL INFO NET/Socket : Using [0]ens97f0:10.10.133.215<0>\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Using non-device net plugin version 0\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Using network Socket\n",
      "frostrend:1297655:1297712 [1] NCCL INFO comm 0x1020af20 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 36000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297711 [0] NCCL INFO comm 0x10201e50 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 35000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297713 [2] NCCL INFO comm 0x10211430 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 37000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297718 [7] NCCL INFO comm 0x10230ca0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId d1000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297715 [4] NCCL INFO comm 0x1021de50 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId ce000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297714 [3] NCCL INFO comm 0x10217940 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 38000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297716 [5] NCCL INFO comm 0x10224360 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId cf000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297717 [6] NCCL INFO comm 0x1022a800 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d0000 commId 0xdb09e0a00213fa7a - Init START\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Setting affinity for GPU 2 to ff00ff\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Setting affinity for GPU 7 to ff00ff00\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Setting affinity for GPU 5 to ff00ff00\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Setting affinity for GPU 0 to ff00ff\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Setting affinity for GPU 3 to ff00ff\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Setting affinity for GPU 6 to ff00ff00\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Setting affinity for GPU 1 to ff00ff\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Setting affinity for GPU 4 to ff00ff00\n",
      "frostrend:1297655:1297714 [3] NCCL INFO comm 0x10217940 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0\n",
      "frostrend:1297655:1297713 [2] NCCL INFO comm 0x10211430 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0\n",
      "frostrend:1297655:1297717 [6] NCCL INFO comm 0x1022a800 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2\n",
      "frostrend:1297655:1297715 [4] NCCL INFO comm 0x1021de50 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0\n",
      "frostrend:1297655:1297712 [1] NCCL INFO comm 0x1020af20 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5\n",
      "frostrend:1297655:1297714 [3] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3\n",
      "frostrend:1297655:1297718 [7] NCCL INFO comm 0x10230ca0 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0\n",
      "frostrend:1297655:1297716 [5] NCCL INFO comm 0x10224360 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0\n",
      "frostrend:1297655:1297717 [6] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1\n",
      "frostrend:1297655:1297711 [0] NCCL INFO comm 0x10201e50 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0\n",
      "frostrend:1297655:1297713 [2] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4\n",
      "frostrend:1297655:1297716 [5] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297715 [4] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297712 [1] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "frostrend:1297655:1297711 [0] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297718 [7] NCCL INFO P2P Chunksize set to 131072\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/direct pointer\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/direct pointer\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/direct pointer\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/direct pointer\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/direct pointer\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/direct pointer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/direct pointer\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Channel 00 : 3[3] -> 4[4] via SHM/direct/direct\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Channel 00 : 7[7] -> 0[0] via SHM/direct/direct\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Channel 01 : 3[3] -> 4[4] via SHM/direct/direct\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Channel 01 : 7[7] -> 0[0] via SHM/direct/direct\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/direct pointer\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/direct pointer\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/direct pointer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/direct pointer\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/direct pointer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/direct pointer\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Connected all rings\n",
      "frostrend:1297655:1297718 [7] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297718 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297718 [7] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/direct pointer\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/direct pointer\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Channel 00 : 4[4] -> 3[3] via SHM/direct/direct\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Channel 01 : 4[4] -> 3[3] via SHM/direct/direct\n",
      "frostrend:1297655:1297711 [0] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297711 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297711 [0] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "frostrend:1297655:1297717 [6] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297717 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297717 [6] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/direct pointer\n",
      "frostrend:1297655:1297716 [5] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297716 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297716 [5] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297715 [4] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297715 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297715 [4] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297712 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297712 [1] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297713 [2] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297713 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297713 [2] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297714 [3] NCCL INFO Connected all trees\n",
      "frostrend:1297655:1297714 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\n",
      "frostrend:1297655:1297714 [3] NCCL INFO 2 coll channels, 0 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "frostrend:1297655:1297712 [1] NCCL INFO comm 0x1020af20 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 36000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297713 [2] NCCL INFO comm 0x10211430 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 37000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297717 [6] NCCL INFO comm 0x1022a800 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d0000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297718 [7] NCCL INFO comm 0x10230ca0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId d1000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297716 [5] NCCL INFO comm 0x10224360 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId cf000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297711 [0] NCCL INFO comm 0x10201e50 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 35000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297714 [3] NCCL INFO comm 0x10217940 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 38000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n",
      "frostrend:1297655:1297715 [4] NCCL INFO comm 0x1021de50 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId ce000 commId 0xdb09e0a00213fa7a - Init COMPLETE\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "test_gpu_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
