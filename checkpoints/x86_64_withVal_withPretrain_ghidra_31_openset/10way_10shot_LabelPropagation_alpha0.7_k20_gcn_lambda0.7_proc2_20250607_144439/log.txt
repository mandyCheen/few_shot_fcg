Device: cuda:2
Model: LabelPropagation(
  (encoder): GCNLayer(
    (gcn_convs): ModuleList(
      (0): GCNConv(128, 256)
      (1): GCNConv(256, 256)
      (2): GCNConv(256, 128)
    )
    (norms): ModuleList(
      (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (relation): GraphRelationNetwork(
    (block): GraphSAGELayer(
      (sage_convs): ModuleList(
        (0): SAGEConv(128, 64, aggr=mean)
        (1): SAGEConv(64, 32, aggr=mean)
      )
      (norms): ModuleList(
        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
      (2): Linear(in_features=16, out_features=1, bias=True)
    )
  )
)
Loss function: LabelPropagation(
  (encoder): GCNLayer(
    (gcn_convs): ModuleList(
      (0): GCNConv(128, 256)
      (1): GCNConv(256, 256)
      (2): GCNConv(256, 128)
    )
    (norms): ModuleList(
      (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (relation): GraphRelationNetwork(
    (block): GraphSAGELayer(
      (sage_convs): ModuleList(
        (0): SAGEConv(128, 64, aggr=mean)
        (1): SAGEConv(64, 32, aggr=mean)
      )
      (norms): ModuleList(
        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
      (2): Linear(in_features=16, out_features=1, bias=True)
    )
  )
)
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Start training...
Epoch 1/200: Avg Train Loss: -0.3310, Avg Train Acc: 0.1769 (Best)
Open-Set AUROC: 0.1134
Epoch 1/200: Avg Val Loss: -0.3722, Avg Val Acc: 0.3127 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 2/200: Avg Train Loss: -0.3572, Avg Train Acc: 0.3174 (Best)
Open-Set AUROC: 0.2728
Epoch 2/200: Avg Val Loss: -0.4821, Avg Val Acc: 0.8552 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 3/200: Avg Train Loss: -0.4648, Avg Train Acc: 0.8660 (Best)
Open-Set AUROC: 0.8871
Epoch 3/200: Avg Val Loss: -0.4978, Avg Val Acc: 0.9233 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 4/200: Avg Train Loss: -0.4898, Avg Train Acc: 0.8894 (Best)
Open-Set AUROC: 0.9169
Epoch 4/200: Avg Val Loss: -0.5118, Avg Val Acc: 0.9217 (Best: 0.9233)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 5/200: Avg Train Loss: -0.5041, Avg Train Acc: 0.9144 (Best)
Open-Set AUROC: 0.9346
Epoch 5/200: Avg Val Loss: -0.5069, Avg Val Acc: 0.9333 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 6/200: Avg Train Loss: -0.5108, Avg Train Acc: 0.9230 (Best)
Open-Set AUROC: 0.9399
Epoch 6/200: Avg Val Loss: -0.5123, Avg Val Acc: 0.9311 (Best: 0.9333)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 7/200: Avg Train Loss: -0.5111, Avg Train Acc: 0.9182 (Best: 0.9230)
Open-Set AUROC: 0.9414
Epoch 7/200: Avg Val Loss: -0.5141, Avg Val Acc: 0.9277 (Best: 0.9333)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 2/20
Epoch 8/200: Avg Train Loss: -0.5177, Avg Train Acc: 0.9188 (Best: 0.9230)
Open-Set AUROC: 0.9396
Epoch 8/200: Avg Val Loss: -0.5138, Avg Val Acc: 0.9338 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 9/200: Avg Train Loss: -0.5171, Avg Train Acc: 0.9201 (Best: 0.9230)
Open-Set AUROC: 0.9498
Epoch 9/200: Avg Val Loss: -0.5174, Avg Val Acc: 0.9271 (Best: 0.9338)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 10/200: Avg Train Loss: -0.5172, Avg Train Acc: 0.9185 (Best: 0.9230)
Open-Set AUROC: 0.9428
Epoch 10/200: Avg Val Loss: -0.5131, Avg Val Acc: 0.9380 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 11/200: Avg Train Loss: -0.5213, Avg Train Acc: 0.9255 (Best)
Open-Set AUROC: 0.9462
Epoch 11/200: Avg Val Loss: -0.5088, Avg Val Acc: 0.9350 (Best: 0.9380)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 12/200: Avg Train Loss: -0.5226, Avg Train Acc: 0.9221 (Best: 0.9255)
Open-Set AUROC: 0.9519
Epoch 12/200: Avg Val Loss: -0.5123, Avg Val Acc: 0.9373 (Best: 0.9380)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 2/20
Epoch 13/200: Avg Train Loss: -0.5235, Avg Train Acc: 0.9252 (Best: 0.9255)
Open-Set AUROC: 0.9511
Epoch 13/200: Avg Val Loss: -0.5105, Avg Val Acc: 0.9354 (Best: 0.9380)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 3/20
Epoch 14/200: Avg Train Loss: -0.5283, Avg Train Acc: 0.9256 (Best)
Open-Set AUROC: 0.9490
Epoch 14/200: Avg Val Loss: -0.5197, Avg Val Acc: 0.9381 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 15/200: Avg Train Loss: -0.5296, Avg Train Acc: 0.9275 (Best)
Open-Set AUROC: 0.9554
Epoch 15/200: Avg Val Loss: -0.5065, Avg Val Acc: 0.9433 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
