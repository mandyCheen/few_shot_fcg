Device: cuda:0
Model: LabelPropagation(
  (encoder): GINLayer(
    (gin_convs): ModuleList(
      (0-2): 3 x GINConv(nn=Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=128, bias=True)
      ))
    )
    (norms): ModuleList(
      (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (mlps): ModuleList(
      (0-2): 3 x Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (relation): GraphRelationNetwork(
    (block): GINLayer(
      (gin_convs): ModuleList(
        (0): GINConv(nn=Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=64, bias=True)
        ))
        (1): GINConv(nn=Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=32, bias=True)
        ))
      )
      (norms): ModuleList(
        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (mlps): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=64, bias=True)
        )
        (1): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=32, bias=True)
        )
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
      (2): Linear(in_features=16, out_features=1, bias=True)
    )
  )
)
Loss function: LabelPropagation(
  (encoder): GINLayer(
    (gin_convs): ModuleList(
      (0-2): 3 x GINConv(nn=Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=128, bias=True)
      ))
    )
    (norms): ModuleList(
      (0-2): 3 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (mlps): ModuleList(
      (0-2): 3 x Sequential(
        (0): Linear(in_features=128, out_features=128, bias=True)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (relation): GraphRelationNetwork(
    (block): GINLayer(
      (gin_convs): ModuleList(
        (0): GINConv(nn=Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=64, bias=True)
        ))
        (1): GINConv(nn=Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=32, bias=True)
        ))
      )
      (norms): ModuleList(
        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (mlps): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=64, bias=True)
        )
        (1): Sequential(
          (0): Linear(in_features=64, out_features=64, bias=True)
          (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU()
          (3): Linear(in_features=64, out_features=32, bias=True)
        )
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
      (2): Linear(in_features=16, out_features=1, bias=True)
    )
  )
)
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Start training...
Epoch 1/200: Avg Train Loss: 0.6909, Avg Train Acc: 0.9056 (Best)
Epoch 1/200: Avg Val Loss: 0.6888, Avg Val Acc: 0.7692 (Best)
Current learning rate: [0.001]
Patience: 0/20
Epoch 2/200: Avg Train Loss: 0.6762, Avg Train Acc: 0.9204 (Best)
Epoch 2/200: Avg Val Loss: 0.6431, Avg Val Acc: 0.9640 (Best)
Current learning rate: [0.001]
Patience: 0/20
Epoch 3/200: Avg Train Loss: 0.6698, Avg Train Acc: 0.9276 (Best)
Epoch 3/200: Avg Val Loss: 0.6413, Avg Val Acc: 0.9592 (Best: 0.9640)
Current learning rate: [0.001]
Patience: 1/20
Epoch 4/200: Avg Train Loss: 0.6720, Avg Train Acc: 0.9248 (Best: 0.9276)
Epoch 4/200: Avg Val Loss: 0.6639, Avg Val Acc: 0.9472 (Best: 0.9640)
Current learning rate: [0.001]
Patience: 2/20
Epoch 5/200: Avg Train Loss: 0.6666, Avg Train Acc: 0.9276 (Best: 0.9276)
Epoch 5/200: Avg Val Loss: 0.6352, Avg Val Acc: 0.9690 (Best)
Current learning rate: [0.001]
Patience: 0/20
Epoch 6/200: Avg Train Loss: 0.6672, Avg Train Acc: 0.9252 (Best: 0.9276)
Epoch 6/200: Avg Val Loss: 0.6480, Avg Val Acc: 0.9510 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 1/20
Epoch 7/200: Avg Train Loss: 0.6683, Avg Train Acc: 0.9218 (Best: 0.9276)
Epoch 7/200: Avg Val Loss: 0.6344, Avg Val Acc: 0.9524 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 2/20
Epoch 8/200: Avg Train Loss: 0.6717, Avg Train Acc: 0.9200 (Best: 0.9276)
Epoch 8/200: Avg Val Loss: 0.6333, Avg Val Acc: 0.9522 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 3/20
Epoch 9/200: Avg Train Loss: 0.6616, Avg Train Acc: 0.9264 (Best: 0.9276)
Epoch 9/200: Avg Val Loss: 0.6378, Avg Val Acc: 0.9558 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 4/20
Epoch 10/200: Avg Train Loss: 0.6629, Avg Train Acc: 0.9184 (Best: 0.9276)
Epoch 10/200: Avg Val Loss: 0.6487, Avg Val Acc: 0.9558 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 5/20
Epoch 11/200: Avg Train Loss: 0.6598, Avg Train Acc: 0.9250 (Best: 0.9276)
Epoch 11/200: Avg Val Loss: 0.6361, Avg Val Acc: 0.9622 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 6/20
Epoch 12/200: Avg Train Loss: 0.6558, Avg Train Acc: 0.9326 (Best)
Epoch 12/200: Avg Val Loss: 0.6390, Avg Val Acc: 0.9528 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 7/20
Epoch 13/200: Avg Train Loss: 0.6503, Avg Train Acc: 0.9438 (Best)
Epoch 13/200: Avg Val Loss: 0.6430, Avg Val Acc: 0.9664 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 8/20
Epoch 14/200: Avg Train Loss: 0.6587, Avg Train Acc: 0.9260 (Best: 0.9438)
Epoch 14/200: Avg Val Loss: 0.6385, Avg Val Acc: 0.9592 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 9/20
Epoch 15/200: Avg Train Loss: 0.6564, Avg Train Acc: 0.9380 (Best: 0.9438)
Epoch 15/200: Avg Val Loss: 0.6356, Avg Val Acc: 0.9594 (Best: 0.9690)
Current learning rate: [0.001]
Patience: 10/20
Epoch 16/200: Avg Train Loss: 0.6557, Avg Train Acc: 0.9356 (Best: 0.9438)
Epoch 16/200: Avg Val Loss: 0.6301, Avg Val Acc: 0.9742 (Best)
Current learning rate: [0.001]
Patience: 0/20
Epoch 17/200: Avg Train Loss: 0.6549, Avg Train Acc: 0.9374 (Best: 0.9438)
Epoch 17/200: Avg Val Loss: 0.6356, Avg Val Acc: 0.9672 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 1/20
Epoch 18/200: Avg Train Loss: 0.6586, Avg Train Acc: 0.9274 (Best: 0.9438)
Epoch 18/200: Avg Val Loss: 0.6362, Avg Val Acc: 0.9574 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 2/20
Epoch 19/200: Avg Train Loss: 0.6561, Avg Train Acc: 0.9294 (Best: 0.9438)
Epoch 19/200: Avg Val Loss: 0.6335, Avg Val Acc: 0.9684 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 3/20
Epoch 20/200: Avg Train Loss: 0.6583, Avg Train Acc: 0.9304 (Best: 0.9438)
Epoch 20/200: Avg Val Loss: 0.6418, Avg Val Acc: 0.9594 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 4/20
Epoch 21/200: Avg Train Loss: 0.6568, Avg Train Acc: 0.9278 (Best: 0.9438)
Epoch 21/200: Avg Val Loss: 0.6404, Avg Val Acc: 0.9618 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 5/20
Epoch 22/200: Avg Train Loss: 0.6586, Avg Train Acc: 0.9248 (Best: 0.9438)
Epoch 22/200: Avg Val Loss: 0.6401, Avg Val Acc: 0.9630 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 6/20
Epoch 23/200: Avg Train Loss: 0.6529, Avg Train Acc: 0.9374 (Best: 0.9438)
Epoch 23/200: Avg Val Loss: 0.6405, Avg Val Acc: 0.9568 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 7/20
Epoch 24/200: Avg Train Loss: 0.6532, Avg Train Acc: 0.9338 (Best: 0.9438)
Epoch 24/200: Avg Val Loss: 0.6359, Avg Val Acc: 0.9628 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 8/20
Epoch 25/200: Avg Train Loss: 0.6569, Avg Train Acc: 0.9326 (Best: 0.9438)
Epoch 25/200: Avg Val Loss: 0.6344, Avg Val Acc: 0.9608 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 9/20
Epoch 26/200: Avg Train Loss: 0.6515, Avg Train Acc: 0.9370 (Best: 0.9438)
Epoch 26/200: Avg Val Loss: 0.6446, Avg Val Acc: 0.9652 (Best: 0.9742)
Current learning rate: [0.001]
Patience: 10/20
Epoch 27/200: Avg Train Loss: 0.6559, Avg Train Acc: 0.9312 (Best: 0.9438)
Epoch 27/200: Avg Val Loss: 0.6441, Avg Val Acc: 0.9618 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 11/20
Epoch 28/200: Avg Train Loss: 0.6572, Avg Train Acc: 0.9312 (Best: 0.9438)
Epoch 28/200: Avg Val Loss: 0.6353, Avg Val Acc: 0.9626 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 12/20
Epoch 29/200: Avg Train Loss: 0.6493, Avg Train Acc: 0.9400 (Best: 0.9438)
Epoch 29/200: Avg Val Loss: 0.6450, Avg Val Acc: 0.9536 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 13/20
Epoch 30/200: Avg Train Loss: 0.6494, Avg Train Acc: 0.9406 (Best: 0.9438)
Epoch 30/200: Avg Val Loss: 0.6390, Avg Val Acc: 0.9574 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 14/20
Epoch 31/200: Avg Train Loss: 0.6501, Avg Train Acc: 0.9378 (Best: 0.9438)
Epoch 31/200: Avg Val Loss: 0.6380, Avg Val Acc: 0.9594 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 15/20
Epoch 32/200: Avg Train Loss: 0.6576, Avg Train Acc: 0.9248 (Best: 0.9438)
Epoch 32/200: Avg Val Loss: 0.6388, Avg Val Acc: 0.9564 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 16/20
Epoch 33/200: Avg Train Loss: 0.6488, Avg Train Acc: 0.9402 (Best: 0.9438)
Epoch 33/200: Avg Val Loss: 0.6444, Avg Val Acc: 0.9562 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 17/20
Epoch 34/200: Avg Train Loss: 0.6476, Avg Train Acc: 0.9350 (Best: 0.9438)
Epoch 34/200: Avg Val Loss: 0.6384, Avg Val Acc: 0.9634 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 18/20
Epoch 35/200: Avg Train Loss: 0.6541, Avg Train Acc: 0.9298 (Best: 0.9438)
Epoch 35/200: Avg Val Loss: 0.6391, Avg Val Acc: 0.9578 (Best: 0.9742)
Current learning rate: [0.0005]
Patience: 19/20
Epoch 36/200: Avg Train Loss: 0.6510, Avg Train Acc: 0.9402 (Best: 0.9438)
Epoch 36/200: Avg Val Loss: 0.6362, Avg Val Acc: 0.9678 (Best: 0.9742)
Current learning rate: [0.0005]
Early stopping in epoch 36
Finish training
