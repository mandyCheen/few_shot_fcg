Device: cuda:0
Model: LabelPropagation(
  (encoder): GCNLayer(
    (gcn_convs): ModuleList(
      (0): GCNConv(128, 256)
      (1): GCNConv(256, 256)
      (2): GCNConv(256, 128)
    )
    (norms): ModuleList(
      (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (relation): GraphRelationNetwork(
    (block): GraphSAGELayer(
      (sage_convs): ModuleList(
        (0): SAGEConv(128, 64, aggr=mean)
        (1): SAGEConv(64, 32, aggr=mean)
      )
      (norms): ModuleList(
        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
      (2): Linear(in_features=16, out_features=1, bias=True)
    )
  )
)
Loss function: LabelPropagation(
  (encoder): GCNLayer(
    (gcn_convs): ModuleList(
      (0): GCNConv(128, 256)
      (1): GCNConv(256, 256)
      (2): GCNConv(256, 128)
    )
    (norms): ModuleList(
      (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (relation): GraphRelationNetwork(
    (block): GraphSAGELayer(
      (sage_convs): ModuleList(
        (0): SAGEConv(128, 64, aggr=mean)
        (1): SAGEConv(64, 32, aggr=mean)
      )
      (norms): ModuleList(
        (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=32, out_features=16, bias=True)
      (1): ReLU()
      (2): Linear(in_features=16, out_features=1, bias=True)
    )
  )
)
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Start training...
Epoch 1/200: Avg Train Loss: 0.2586, Avg Train Acc: 0.8460 (Best)
Open-Set AUROC: 0.8017
Epoch 1/200: Avg Val Loss: 0.2343, Avg Val Acc: 0.9071 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 2/200: Avg Train Loss: 0.1931, Avg Train Acc: 0.9231 (Best)
Open-Set AUROC: 0.9310
Epoch 2/200: Avg Val Loss: 0.2264, Avg Val Acc: 0.9070 (Best: 0.9071)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 3/200: Avg Train Loss: 0.1796, Avg Train Acc: 0.9324 (Best)
Open-Set AUROC: 0.9438
Epoch 3/200: Avg Val Loss: 0.2121, Avg Val Acc: 0.8924 (Best: 0.9071)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 2/20
Epoch 4/200: Avg Train Loss: 0.1727, Avg Train Acc: 0.9394 (Best)
Open-Set AUROC: 0.9552
Epoch 4/200: Avg Val Loss: 0.2025, Avg Val Acc: 0.9115 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 5/200: Avg Train Loss: 0.1704, Avg Train Acc: 0.9403 (Best)
Open-Set AUROC: 0.9551
Epoch 5/200: Avg Val Loss: 0.1905, Avg Val Acc: 0.9150 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 6/200: Avg Train Loss: 0.1654, Avg Train Acc: 0.9393 (Best: 0.9403)
Open-Set AUROC: 0.9553
Epoch 6/200: Avg Val Loss: 0.1971, Avg Val Acc: 0.9130 (Best: 0.9150)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 7/200: Avg Train Loss: 0.1633, Avg Train Acc: 0.9372 (Best: 0.9403)
Open-Set AUROC: 0.9558
Epoch 7/200: Avg Val Loss: 0.1933, Avg Val Acc: 0.9182 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 8/200: Avg Train Loss: 0.1657, Avg Train Acc: 0.9358 (Best: 0.9403)
Open-Set AUROC: 0.9578
Epoch 8/200: Avg Val Loss: 0.2000, Avg Val Acc: 0.9246 (Best)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 0/20
Epoch 9/200: Avg Train Loss: 0.1613, Avg Train Acc: 0.9426 (Best)
Open-Set AUROC: 0.9605
Epoch 9/200: Avg Val Loss: 0.1989, Avg Val Acc: 0.9158 (Best: 0.9246)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 1/20
Epoch 10/200: Avg Train Loss: 0.1600, Avg Train Acc: 0.9428 (Best)
Open-Set AUROC: 0.9581
Epoch 10/200: Avg Val Loss: 0.2053, Avg Val Acc: 0.9211 (Best: 0.9246)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 2/20
Epoch 11/200: Avg Train Loss: 0.1602, Avg Train Acc: 0.9406 (Best: 0.9428)
Open-Set AUROC: 0.9587
Epoch 11/200: Avg Val Loss: 0.2111, Avg Val Acc: 0.9123 (Best: 0.9246)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 3/20
Epoch 12/200: Avg Train Loss: 0.1572, Avg Train Acc: 0.9472 (Best)
Open-Set AUROC: 0.9611
Epoch 12/200: Avg Val Loss: 0.2158, Avg Val Acc: 0.9147 (Best: 0.9246)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 4/20
Epoch 13/200: Avg Train Loss: 0.1539, Avg Train Acc: 0.9504 (Best)
Open-Set AUROC: 0.9586
Epoch 13/200: Avg Val Loss: 0.2064, Avg Val Acc: 0.9166 (Best: 0.9246)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 5/20
Epoch 14/200: Avg Train Loss: 0.1557, Avg Train Acc: 0.9428 (Best: 0.9504)
Open-Set AUROC: 0.9649
Epoch 14/200: Avg Val Loss: 0.1980, Avg Val Acc: 0.9147 (Best: 0.9246)
Open-Set AUROC: nan
Current learning rate: [0.001]
Patience: 6/20
