Pretrained model loaded from ./pretrained/x86_pretrained_20241121_1653/epoch_2060_best_backbone.pth
Device: cuda:0
Model: GraphSAGELayer(
  (sage_convs): ModuleList(
    (0-1): 2 x SAGEConv(128, 128, aggr=mean)
  )
  (norms): ModuleList(
    (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
)
Loss function: LabelPropagation(
  (relation): GraphRelationNetwork(
    (sage): GraphSAGELayer(
      (sage_convs): ModuleList(
        (0): SAGEConv(128, 128, aggr=mean)
      )
      (norms): ModuleList(
        (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (fc): Sequential(
      (0): Linear(in_features=128, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=1, bias=True)
    )
  )
)
Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
)
Start training...
Epoch 1/500: Avg Train Loss: 2.2987, Avg Train Acc: 0.1981 (Best)
Epoch 1/500: Avg Val Loss: 2.6717, Avg Val Acc: 0.1987 (Best)
Patience: 0/20
Epoch 2/500: Avg Train Loss: 2.2667, Avg Train Acc: 0.1985 (Best)
Epoch 2/500: Avg Val Loss: 2.4775, Avg Val Acc: 0.1995 (Best)
Patience: 0/20
Epoch 3/500: Avg Train Loss: 2.1934, Avg Train Acc: 0.1997 (Best)
Epoch 3/500: Avg Val Loss: 2.4880, Avg Val Acc: 0.1998 (Best)
Patience: 0/20
